{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":347,"status":"ok","timestamp":1697342783939,"user":{"displayName":"Leyuan Yang","userId":"16257527932374764090"},"user_tz":-480},"id":"eXids5mq8b5E","outputId":"f0318977-de3b-4340-aa90-d4cb1109369a"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7fb9d229ca30>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from __future__ import print_function, division\n","import numpy as np\n","import pandas as pd\n","\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_fscore_support, f1_score, classification_report, roc_auc_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","import pickle\n","import networkx as nx\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# import warnings\n","# warnings.filterwarnings(\"ignore\")\n","torch.manual_seed(15)"]},{"cell_type":"markdown","metadata":{"id":"A5G2FFj88b5H"},"source":["# Import data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def load_pickle(fname):\n","    with open(fname, 'rb') as f:\n","        return pickle.load(f)\n","\n","G = load_pickle('subgraph.pkl')\n","G_simple = nx.Graph(G)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>node</th>\n","      <th>isp</th>\n","      <th>closeness_centrality</th>\n","      <th>betweenness_centrality</th>\n","      <th>eigenvector_centrality</th>\n","      <th>active_days</th>\n","      <th>eccentricity</th>\n","      <th>pagerank_std_last_month</th>\n","      <th>in_out_degree_ratio</th>\n","      <th>weightsin_weightsout_ratio</th>\n","      <th>numin_numout_ratio</th>\n","      <th>centrality_sum</th>\n","      <th>rolling_average_pagerank</th>\n","      <th>cumulative_interaction_count</th>\n","      <th>pagerank_change</th>\n","      <th>pagerank_closeness_interaction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0x1f1e784a61a8ca0a90250bcd2170696655b28a21</td>\n","      <td>0</td>\n","      <td>0.132361</td>\n","      <td>2.290699e-04</td>\n","      <td>0.002497</td>\n","      <td>107.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>0.132389</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>0.135087</td>\n","      <td>0.000000e+00</td>\n","      <td>1327.0</td>\n","      <td>0.000000e+00</td>\n","      <td>8.164333e-08</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0x1266f8b9e4dffc9e2f719bf51713f7e714516861</td>\n","      <td>0</td>\n","      <td>0.109312</td>\n","      <td>1.747367e-07</td>\n","      <td>0.000018</td>\n","      <td>1.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>0.109330</td>\n","      <td>0.000000e+00</td>\n","      <td>1328.0</td>\n","      <td>-4.834848e-07</td>\n","      <td>1.457527e-08</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0xbbfaf27674c2eb5d13edc58a40081248d13dcfeb</td>\n","      <td>1</td>\n","      <td>0.117714</td>\n","      <td>7.214720e-05</td>\n","      <td>0.000071</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>0.117857</td>\n","      <td>5.317118e-07</td>\n","      <td>0.0</td>\n","      <td>7.116401e-07</td>\n","      <td>9.946543e-08</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0xb50d0c4cb2c29cc232c96a59e9c65eb82914ec75</td>\n","      <td>0</td>\n","      <td>0.110925</td>\n","      <td>1.036793e-04</td>\n","      <td>0.002118</td>\n","      <td>89.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>0.379518</td>\n","      <td>0.434854</td>\n","      <td>1.0</td>\n","      <td>0.113146</td>\n","      <td>6.202811e-07</td>\n","      <td>1445.0</td>\n","      <td>7.495447e-07</td>\n","      <td>9.791388e-08</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0x563b377a956c80d77a7c613a9343699ad6123911</td>\n","      <td>0</td>\n","      <td>0.110763</td>\n","      <td>5.919153e-03</td>\n","      <td>0.006486</td>\n","      <td>346.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>0.010035</td>\n","      <td>0.000029</td>\n","      <td>1.0</td>\n","      <td>0.123168</td>\n","      <td>4.186281e-07</td>\n","      <td>9354.0</td>\n","      <td>-7.983363e-07</td>\n","      <td>1.538709e-08</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         node  isp  closeness_centrality  \\\n","0  0x1f1e784a61a8ca0a90250bcd2170696655b28a21    0              0.132361   \n","1  0x1266f8b9e4dffc9e2f719bf51713f7e714516861    0              0.109312   \n","2  0xbbfaf27674c2eb5d13edc58a40081248d13dcfeb    1              0.117714   \n","3  0xb50d0c4cb2c29cc232c96a59e9c65eb82914ec75    0              0.110925   \n","4  0x563b377a956c80d77a7c613a9343699ad6123911    0              0.110763   \n","\n","   betweenness_centrality  eigenvector_centrality  active_days  eccentricity  \\\n","0            2.290699e-04                0.002497        107.0           7.0   \n","1            1.747367e-07                0.000018          1.0           8.0   \n","2            7.214720e-05                0.000071          0.0           7.0   \n","3            1.036793e-04                0.002118         89.0           7.0   \n","4            5.919153e-03                0.006486        346.0           7.0   \n","\n","   pagerank_std_last_month  in_out_degree_ratio  weightsin_weightsout_ratio  \\\n","0                      0.0             0.132389                    1.000000   \n","1                      0.0             1.000000                    1.000000   \n","2                      0.0             1.000000                    1.000000   \n","3                      0.0             0.379518                    0.434854   \n","4                      0.0             0.010035                    0.000029   \n","\n","   numin_numout_ratio  centrality_sum  rolling_average_pagerank  \\\n","0                 1.0        0.135087              0.000000e+00   \n","1                 1.0        0.109330              0.000000e+00   \n","2                 0.0        0.117857              5.317118e-07   \n","3                 1.0        0.113146              6.202811e-07   \n","4                 1.0        0.123168              4.186281e-07   \n","\n","   cumulative_interaction_count  pagerank_change  \\\n","0                        1327.0     0.000000e+00   \n","1                        1328.0    -4.834848e-07   \n","2                           0.0     7.116401e-07   \n","3                        1445.0     7.495447e-07   \n","4                        9354.0    -7.983363e-07   \n","\n","   pagerank_closeness_interaction  \n","0                    8.164333e-08  \n","1                    1.457527e-08  \n","2                    9.946543e-08  \n","3                    9.791388e-08  \n","4                    1.538709e-08  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_features = pd.read_csv(\"feature_data.csv\")\n","df_features.dropna(inplace=True)\n","df_features.head(5)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def prepare_data(graph, df_features, sample = True, sample_num = 500):\n","    df_class = df_features[[\"node\", \"isp\"]]\n","\n","    # Create a mapping from unique strings to unique integers\n","    string_to_int = {string: index for index, string in enumerate(set(df_features['node']))}\n","\n","    # Use the mapping to replace strings with integers in the list\n","    int_list = [string_to_int[value] for value in df_features['node']]\n","    df_features['Node'] = int_list\n","\n","    df_features.drop(columns=['node'], inplace=True)\n","\n","    # Map values in the 'Node' column to the df_class\n","    df_class['Node'] = df_class['node'].map(string_to_int)\n","\n","    # Drop rows where the mapping is NaN (not found in the dictionary)\n","    df_class = df_class.dropna(subset=['Node'])\n","\n","    # Optionally, drop the original 'node' column if not needed\n","    df_class = df_class.drop(columns=['node'])\n","\n","    adj_m = nx.to_pandas_adjacency(graph)\n","\n","    # Map values in the 'Node' column to the dictionary\n","    adj_m['Node'] = adj_m.index.map(string_to_int)\n","    adj_m.dropna(inplace=True)\n","    adj_m['Node'] = adj_m['Node'].astype(int)\n","\n","    # Drop rows where the mapping is NaN (not found in the dictionary)\n","    adj_m = adj_m.dropna(subset=['Node'])\n","\n","    # Optionally, drop the original 'fruit' column if not needed\n","    adj_m.set_index(adj_m['Node'], inplace=True)\n","\n","    adj_m = adj_m.rename(columns=string_to_int)\n","    adj_m = adj_m.drop(columns=['Node'])\n","    edg = list(adj_m.index)\n","\n","    feature_filtered = df_features[df_features['Node'].isin(edg)]\n","\n","    if sample:\n","        feature_filtered = pd.concat([\n","            feature_filtered[feature_filtered['isp'] == 0].sample(sample_num),\n","            feature_filtered[feature_filtered['isp'] == 1].sample(sample_num)\n","        ])\n","\n","        selected_nodes = feature_filtered['Node'].tolist()\n","        feature_filtered.drop(columns=['isp'],inplace=True)\n","\n","        adj_m = adj_m[adj_m.index.isin(selected_nodes)]\n","        adj_m = adj_m.loc[:, selected_nodes]\n","        df_class = df_class[df_class['Node'].isin(selected_nodes)]\n","\n","    classes_ts = df_class.sort_values(by='Node')\n","    feature_filtered = feature_filtered.sort_values(by='Node')\n","    adj_mats = adj_m.sort_index(axis=0).sort_index(axis=1)\n","\n","    adj_np = np.array(adj_mats, dtype=np.float_)\n","\n","    labels_ts = np.array(classes_ts['isp'] == 0, dtype = np.longlong)\n","    adj = torch.tensor(adj_np)\n","    features = torch.tensor(feature_filtered.values)\n","    lables = torch.tensor(labels_ts, dtype = torch.long)\n","\n","    return labels_ts, adj, features"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/h3/d_dx8gd966jdtf5gj5w2y04h0000gn/T/ipykernel_7348/4276469871.py:14: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_class['Node'] = df_class['node'].map(string_to_int)\n"]}],"source":["labels, adj, features = prepare_data(G_simple, df_features)"]},{"cell_type":"markdown","metadata":{"id":"7jkP1cUrCuPk"},"source":["# GCN"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Zv7AZ5eDCwn4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import time\n","import sys\n","\n","class GraphConv(nn.Module):\n","    def __init__(self, in_features, out_features, activation  = 'relu', skip = False, skip_in_features = None):\n","        super(GraphConv, self).__init__()\n","        self.W = torch.nn.Parameter(torch.DoubleTensor(in_features, out_features))\n","        nn.init.xavier_uniform_(self.W)\n","\n","        self.set_act = False\n","        if activation == 'relu':\n","            self.activation = nn.ReLU()\n","            self.set_act = True\n","        elif activation == 'softmax':\n","            self.activation = nn.Softmax(dim = 1)\n","            self.set_act = True\n","        else:\n","            self.set_act = False\n","            raise ValueError(\"activations supported are 'relu' and 'softmax'\")\n","\n","        self.skip = skip\n","        if self.skip:\n","            if skip_in_features == None:\n","                raise ValueError(\"pass input feature size of the skip connection\")\n","            self.W_skip = torch.nn.Parameter(torch.DoubleTensor(skip_in_features, out_features))\n","            nn.init.xavier_uniform_(self.W)\n","\n","    def forward(self, A, H_in, H_skip_in = None):\n","        # A must be an n x n matrix as it is an adjacency matrix\n","        # H is the input of the node embeddings, shape will n x in_features\n","        self.A = A\n","        self.H_in = H_in\n","        A_ = torch.add(self.A, torch.eye(self.A.shape[0]).double())\n","        D_ = torch.diag(A_.sum(1))\n","        # since D_ is a diagonal matrix,\n","        # its root will be the roots of the diagonal elements on the principle diagonal\n","        # since A is an adjacency matrix, we are only dealing with positive values\n","        # all roots will be real\n","        D_root_inv = torch.inverse(torch.sqrt(D_))\n","        A_norm = torch.mm(torch.mm(D_root_inv, A_), D_root_inv)\n","        # shape of A_norm will be n x n\n","\n","        H_out = torch.mm(torch.mm(A_norm, H_in), self.W)\n","        # shape of H_out will be n x out_features\n","\n","        if self.skip:\n","            H_skip_out = torch.mm(H_skip_in, self.W_skip)\n","            H_out = torch.add(H_out, H_skip_out)\n","\n","        if self.set_act:\n","            H_out = self.activation(H_out)\n","\n","        return H_out"]},{"cell_type":"markdown","metadata":{"id":"mUlfyngFC0fC"},"source":["# GCN 2 layers"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"I56fo6GCC2Ix"},"outputs":[],"source":["class GCN_2layer(nn.Module):\n","    def __init__(self, in_features, hidden_features, out_features, skip = False):\n","        super(GCN_2layer, self).__init__()\n","        self.skip = skip\n","\n","        self.gcl1 = GraphConv(in_features, hidden_features)\n","\n","        if self.skip:\n","            self.gcl_skip = GraphConv(hidden_features, out_features, activation = 'softmax', skip = self.skip,\n","                                  skip_in_features = in_features)\n","        else:\n","            self.gcl2 = GraphConv(hidden_features, out_features, activation = 'softmax')\n","\n","    def forward(self, A, X):\n","        out = self.gcl1(A, X)\n","        if self.skip:\n","            out = self.gcl_skip(A, out, X)\n","        else:\n","            out = self.gcl2(A, out)\n","\n","        return out"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397747,"status":"ok","timestamp":1697344468300,"user":{"displayName":"Leyuan Yang","userId":"16257527932374764090"},"user_tz":-480},"id":"B9SF2UahC9k-","outputId":"3cd2e682-7f80-4043-f622-74749bcdf0c9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/h3/d_dx8gd966jdtf5gj5w2y04h0000gn/T/ipykernel_7348/3241918005.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  A = torch.tensor(adj)\n","/var/folders/h3/d_dx8gd966jdtf5gj5w2y04h0000gn/T/ipykernel_7348/3241918005.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  X = torch.tensor(features)\n"]},{"name":"stdout","output_type":"stream","text":[" Epoch 14/15 Timestamp 1/34 training loss: 1.013262 training accuracy: 0.500000 Time: 0.10499405860900879"]}],"source":["num_features = 15\n","num_classes = 2\n","num_ts = 49\n","epochs = 15\n","lr = 0.001\n","max_train_ts = 34\n","train_ts = np.arange(max_train_ts)\n","\n","#adj_mats, features_labelled_ts, classes_ts = dataSet\n","\n","# 0 - illicit, 1 - licit\n","# labels_ts = []\n","# for c in classes_ts:\n","#     labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n","\n","gcn = GCN_2layer(num_features, 100, num_classes)\n","train_loss = nn.CrossEntropyLoss(weight = torch.DoubleTensor([0.7, 0.3]))\n","optimizer = torch.optim.Adam(gcn.parameters(), lr = lr)\n","\n","# Training\n","\n","# for ts in train_ts:\n","A = torch.tensor(adj)\n","X = torch.tensor(features)\n","L = torch.tensor(labels, dtype = torch.long)\n","for ep in range(epochs):\n","    t_start = time.time()\n","\n","    gcn.train()\n","    optimizer.zero_grad()\n","    out = gcn(A, X)  \n","\n","    loss = train_loss(out, L)\n","    train_pred = out.max(1)[1].type_as(L)\n","    acc = (train_pred.eq(L).double().sum())/L.shape[0]\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    sys.stdout.write(\"\\r Epoch %d/%d Timestamp %d/%d training loss: %f training accuracy: %f Time: %s\"\n","                        %(ep, epochs, 1, max_train_ts, loss, acc, time.time() - t_start)\n","                    )\n","\n","torch.save(gcn.state_dict(), \"modelDir\"+ \"gcn_weights.pth\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37226,"status":"ok","timestamp":1697344756356,"user":{"displayName":"Leyuan Yang","userId":"16257527932374764090"},"user_tz":-480},"id":"ys1YO5VZDV3j","outputId":"caaf126b-e3b4-4935-ca3b-53a7ad3c3207"},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN - averaged accuracy: 0.5, precision: 0.5, recall: 1.0, f1: 0.6666666666666666\n","ROC AUC: 0.5000\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       500\n","           1       0.50      1.00      0.67       500\n","\n","    accuracy                           0.50      1000\n","   macro avg       0.25      0.50      0.33      1000\n","weighted avg       0.25      0.50      0.33      1000\n","\n"]},{"name":"stderr","output_type":"stream","text":["/Users/yco/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/Users/yco/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/Users/yco/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.metrics import f1_score, precision_score, recall_score\n","test_ts = np.arange(14)\n","# adj_mats, features_labelled_ts, classes_ts = load_data(df_classes,df_edges,df_features, 35, 49)\n","\n","# 0 - illicit, 1 - licit\n","# labels_ts = []\n","# for c in classes_ts:\n","#     labels_ts.append(np.array(c['class'] == '2', dtype = np.long))\n","\n","gcn = GCN_2layer(num_features, 100, num_classes)\n","gcn.load_state_dict(torch.load(\"modelDir\"+ \"gcn_weights.pth\"))\n","\n","# Testing\n","test_accs = []\n","test_precisions = []\n","test_recalls = []\n","test_f1s = []\n","\n","# for ts in test_ts:\n","# A = torch.tensor(adj_mats.values)\n","# X = torch.tensor(features_labelled_ts.values)\n","# L = torch.tensor(labels_ts, dtype = torch.long)\n","\n","gcn.eval()\n","test_out = gcn(A, X)\n","\n","test_pred = test_out.max(1)[1].type_as(L)\n","t_acc = (test_pred.eq(L).double().sum())/L.shape[0]\n","test_accs.append(t_acc.item())\n","test_precisions.append(precision_score(L, test_pred))\n","test_recalls.append(recall_score(L, test_pred))\n","test_f1s.append(f1_score(L, test_pred))\n","\n","acc = np.array(test_accs).mean()\n","prec = np.array(test_precisions).mean()\n","rec = np.array(test_recalls).mean()\n","f1 = np.array(test_f1s).mean()\n","\n","# Calculate ROC AUC\n","roc_auc = roc_auc_score(L, test_out[:, 1].detach().numpy())  # Use probabilities for ROC AUC, assuming binary classification\n","\n","# Generate classification report\n","class_report = classification_report(L, test_pred.detach().numpy())  # Assuming L and test_pred are tensors\n","\n","\n","\n","print(\"GCN - averaged accuracy: {}, precision: {}, recall: {}, f1: {}\".format(acc, prec, rec, f1))\n","print(\"ROC AUC: {:.4f}\".format(roc_auc))\n","print(\"Classification Report:\\n\", class_report)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/cocoy02/BT4012_AML-Fraud-Detection/blob/main/GCN.ipynb","timestamp":1696746075822}]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
